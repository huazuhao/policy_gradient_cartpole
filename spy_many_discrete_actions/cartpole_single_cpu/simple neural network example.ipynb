{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fee243b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy loss is tensor(94.1372, device='cuda:0', grad_fn=<AddBackward0>)\n",
      " Epoch: 1, Avg Return per Epoch: 17.000policy loss is tensor(133.0694, device='cuda:0', grad_fn=<AddBackward0>)\n",
      " Epoch: 2, Avg Return per Epoch: 18.750policy loss is tensor(62.5724, device='cuda:0', grad_fn=<AddBackward0>)\n",
      " Epoch: 3, Avg Return per Epoch: 17.333policy loss is tensor(781.6501, device='cuda:0', grad_fn=<AddBackward0>)\n",
      " Epoch: 4, Avg Return per Epoch: 23.750"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zuhao/anaconda2/envs/p37/lib/python3.7/site-packages/ipykernel_launcher.py:179: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "/home/zuhao/anaconda2/envs/p37/lib/python3.7/site-packages/ipykernel_launcher.py:180: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "/home/zuhao/anaconda2/envs/p37/lib/python3.7/site-packages/ipykernel_launcher.py:286: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy loss is tensor(67.6004, device='cuda:0', grad_fn=<AddBackward0>)\n",
      " Epoch: 5, Avg Return per Epoch: 21.900policy loss is tensor(179.4578, device='cuda:0', grad_fn=<AddBackward0>)\n",
      " Epoch: 6, Avg Return per Epoch: 22.333policy loss is tensor(127.6544, device='cuda:0', grad_fn=<AddBackward0>)\n",
      " Epoch: 7, Avg Return per Epoch: 21.857policy loss is tensor(64.5600, device='cuda:0', grad_fn=<AddBackward0>)\n",
      " Epoch: 8, Avg Return per Epoch: 20.938policy loss is tensor(153.4028, device='cuda:0', grad_fn=<AddBackward0>)\n",
      " Epoch: 9, Avg Return per Epoch: 21.167policy loss is tensor(58.4239, device='cuda:0', grad_fn=<AddBackward0>)\n",
      " Epoch: 10, Avg Return per Epoch: 20.350policy loss is tensor(93.4134, device='cuda:0', grad_fn=<AddBackward0>)\n",
      " Epoch: 11, Avg Return per Epoch: 20.000policy loss is tensor(242.1928, device='cuda:0', grad_fn=<AddBackward0>)\n",
      " Epoch: 12, Avg Return per Epoch: 20.500policy loss is tensor(758.4088, device='cuda:0', grad_fn=<AddBackward0>)\n",
      " Epoch: 13, Avg Return per Epoch: 23.038policy loss is tensor(289.3066, device='cuda:0', grad_fn=<AddBackward0>)\n",
      " Epoch: 14, Avg Return per Epoch: 23.500policy loss is tensor(1535.4800, device='cuda:0', grad_fn=<AddBackward0>)\n",
      " Epoch: 15, Avg Return per Epoch: 26.500policy loss is tensor(466.7549, device='cuda:0', grad_fn=<AddBackward0>)\n",
      " Epoch: 16, Avg Return per Epoch: 27.125policy loss is tensor(1240.6803, device='cuda:0', grad_fn=<AddBackward0>)\n",
      " Epoch: 17, Avg Return per Epoch: 29.618policy loss is tensor(691.3453, device='cuda:0', grad_fn=<AddBackward0>)\n",
      " Epoch: 18, Avg Return per Epoch: 30.806policy loss is tensor(1378.8969, device='cuda:0', grad_fn=<AddBackward0>)\n",
      " Epoch: 19, Avg Return per Epoch: 33.026policy loss is tensor(2409.3076, device='cuda:0', grad_fn=<AddBackward0>)\n",
      " Epoch: 20, Avg Return per Epoch: 36.575policy loss is tensor(397.6606, device='cuda:0', grad_fn=<AddBackward0>)\n",
      " Epoch: 21, Avg Return per Epoch: 36.524policy loss is tensor(586.4564, device='cuda:0', grad_fn=<AddBackward0>)\n",
      " Epoch: 22, Avg Return per Epoch: 37.068policy loss is tensor(627.4222, device='cuda:0', grad_fn=<AddBackward0>)\n",
      " Epoch: 23, Avg Return per Epoch: 37.478policy loss is tensor(468.0071, device='cuda:0', grad_fn=<AddBackward0>)\n",
      " Epoch: 24, Avg Return per Epoch: 37.750policy loss is tensor(311.9852, device='cuda:0', grad_fn=<AddBackward0>)\n",
      " Epoch: 25, Avg Return per Epoch: 37.740policy loss is tensor(182.6167, device='cuda:0', grad_fn=<AddBackward0>)\n",
      " Epoch: 26, Avg Return per Epoch: 37.288policy loss is tensor(108.5203, device='cuda:0', grad_fn=<AddBackward0>)\n",
      " Epoch: 27, Avg Return per Epoch: 36.704policy loss is tensor(309.5121, device='cuda:0', grad_fn=<AddBackward0>)\n",
      " Epoch: 28, Avg Return per Epoch: 36.536policy loss is tensor(455.9030, device='cuda:0', grad_fn=<AddBackward0>)\n",
      " Epoch: 29, Avg Return per Epoch: 36.741policy loss is tensor(757.1486, device='cuda:0', grad_fn=<AddBackward0>)\n",
      " Epoch: 30, Avg Return per Epoch: 37.233policy loss is tensor(751.0283, device='cuda:0', grad_fn=<AddBackward0>)\n",
      " Epoch: 31, Avg Return per Epoch: 37.903policy loss is tensor(473.0232, device='cuda:0', grad_fn=<AddBackward0>)\n",
      " Epoch: 32, Avg Return per Epoch: 38.109policy loss is tensor(1244.2499, device='cuda:0', grad_fn=<AddBackward0>)\n",
      " Epoch: 33, Avg Return per Epoch: 39.091policy loss is tensor(4531.3452, device='cuda:0', grad_fn=<AddBackward0>)\n",
      " Epoch: 34, Avg Return per Epoch: 42.471policy loss is tensor(5628.9951, device='cuda:0', grad_fn=<AddBackward0>)\n",
      " Epoch: 35, Avg Return per Epoch: 46.671policy loss is tensor(3212.5498, device='cuda:0', grad_fn=<AddBackward0>)\n",
      " Epoch: 36, Avg Return per Epoch: 49.403policy loss is tensor(3365.9087, device='cuda:0', grad_fn=<AddBackward0>)\n",
      " Epoch: 37, Avg Return per Epoch: 51.622policy loss is tensor(6706.4028, device='cuda:0', grad_fn=<AddBackward0>)\n",
      " Epoch: 38, Avg Return per Epoch: 56.316policy loss is tensor(7833.3735, device='cuda:0', grad_fn=<AddBackward0>)\n",
      " Epoch: 39, Avg Return per Epoch: 60.987policy loss is tensor(8336.4365, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-8ee3fed47214>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;31m#     args = parser.parse_args()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;31m#     env = args.env\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-8ee3fed47214>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[0mpolicy_gradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPolicyGradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproblem\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cuda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m     \u001b[0mpolicy_gradient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve_environment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-8ee3fed47214>\u001b[0m in \u001b[0;36msolve_environment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0;31m# backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \u001b[0;31m# update the parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/p37/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/p37/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.functional import one_hot, log_softmax, softmax, normalize\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import deque\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--env', help='CartPole or LunarLander OpenAI gym environment', type=str)\n",
    "parser.add_argument('--use_cuda', help='Use if you want to use CUDA', action='store_true')\n",
    "\n",
    "\n",
    "class Params:\n",
    "    NUM_EPOCHS = 10\n",
    "    ALPHA = 5e-3        # learning rate\n",
    "    BATCH_SIZE = 2      # how many episodes we want to pack into an epoch\n",
    "    GAMMA = 0.99        # discount rate\n",
    "    HIDDEN_SIZE = 64    # number of hidden nodes we have in our dnn\n",
    "    BETA = 0.1          # the entropy bonus multiplier\n",
    "\n",
    "\n",
    "# Q-table is replaced by a neural network\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, observation_space_size: int, action_space_size: int, hidden_size: int):\n",
    "        super(Agent, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features=observation_space_size, out_features=hidden_size, bias=True),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(in_features=hidden_size, out_features=hidden_size, bias=True),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(in_features=hidden_size, out_features=action_space_size, bias=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = normalize(x, dim=1)\n",
    "        x = self.net(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PolicyGradient:\n",
    "    def __init__(self, problem: str = \"CartPole\", use_cuda: bool = False):\n",
    "\n",
    "        self.NUM_EPOCHS = Params.NUM_EPOCHS\n",
    "        self.ALPHA = Params.ALPHA\n",
    "        self.BATCH_SIZE = Params.BATCH_SIZE\n",
    "        self.GAMMA = Params.GAMMA\n",
    "        self.HIDDEN_SIZE = Params.HIDDEN_SIZE\n",
    "        self.BETA = Params.BETA\n",
    "        self.DEVICE = torch.device('cuda' if torch.cuda.is_available() and use_cuda else 'cpu')\n",
    "\n",
    "        # instantiate the tensorboard writer\n",
    "        self.writer = SummaryWriter(comment=f'_PG_CP_Gamma={self.GAMMA},'\n",
    "                                            f'LR={self.ALPHA},'\n",
    "                                            f'BS={self.BATCH_SIZE},'\n",
    "                                            f'NH={self.HIDDEN_SIZE},'\n",
    "                                            f'BETA={self.BETA}')\n",
    "\n",
    "        # create the environment\n",
    "        self.env = gym.make('CartPole-v1') if problem == \"CartPole\" else gym.make('LunarLander-v2')\n",
    "\n",
    "        # the agent driven by a neural network architecture\n",
    "        self.agent = Agent(observation_space_size=self.env.observation_space.shape[0],\n",
    "                           action_space_size=self.env.action_space.n,\n",
    "                           hidden_size=self.HIDDEN_SIZE).to(self.DEVICE)\n",
    "\n",
    "        self.adam = optim.Adam(params=self.agent.parameters(), lr=self.ALPHA)\n",
    "\n",
    "        self.total_rewards = deque([], maxlen=200)\n",
    "\n",
    "        # flag to figure out if we have render a single episode current epoch\n",
    "        self.finished_rendering_this_epoch = False\n",
    "\n",
    "    def solve_environment(self):\n",
    "        \"\"\"\n",
    "            The main interface for the Policy Gradient solver\n",
    "        \"\"\"\n",
    "        # init the episode and the epoch\n",
    "        episode = 0\n",
    "        epoch = 0\n",
    "\n",
    "        # init the epoch arrays\n",
    "        # used for entropy calculation\n",
    "        epoch_logits = torch.empty(size=(0, self.env.action_space.n), device=self.DEVICE)\n",
    "        epoch_weighted_log_probs = torch.empty(size=(0,), dtype=torch.float, device=self.DEVICE)\n",
    "\n",
    "        while True:\n",
    "\n",
    "            # play an episode of the environment\n",
    "            (episode_weighted_log_prob_trajectory,\n",
    "             episode_logits,\n",
    "             sum_of_episode_rewards,\n",
    "             episode) = self.play_episode(episode=episode)\n",
    "\n",
    "            # after each episode append the sum of total rewards to the deque\n",
    "            self.total_rewards.append(sum_of_episode_rewards)\n",
    "\n",
    "            # append the weighted log-probabilities of actions\n",
    "            epoch_weighted_log_probs = torch.cat((epoch_weighted_log_probs, episode_weighted_log_prob_trajectory),\n",
    "                                                 dim=0)\n",
    "\n",
    "            # append the logits - needed for the entropy bonus calculation\n",
    "            epoch_logits = torch.cat((epoch_logits, episode_logits), dim=0)\n",
    "\n",
    "            # if the epoch is over - we have epoch trajectories to perform the policy gradient\n",
    "            if episode >= self.BATCH_SIZE:\n",
    "\n",
    "                # reset the rendering flag\n",
    "                self.finished_rendering_this_epoch = False\n",
    "\n",
    "                # reset the episode count\n",
    "                episode = 0\n",
    "\n",
    "                # increment the epoch\n",
    "                epoch += 1\n",
    "\n",
    "                # calculate the loss\n",
    "                loss, entropy = self.calculate_loss(epoch_logits=epoch_logits,\n",
    "                                                    weighted_log_probs=epoch_weighted_log_probs)\n",
    "\n",
    "                # zero the gradient\n",
    "                self.adam.zero_grad()\n",
    "\n",
    "                # backprop\n",
    "                loss.backward()\n",
    "\n",
    "                # update the parameters\n",
    "                self.adam.step()\n",
    "\n",
    "                # feedback\n",
    "                print(\"\\r\", f\"Epoch: {epoch}, Avg Return per Epoch: {np.mean(self.total_rewards):.3f}\",\n",
    "                      end=\"\",\n",
    "                      flush=True)\n",
    "\n",
    "                self.writer.add_scalar(tag='Average Return over 100 episodes',\n",
    "                                       scalar_value=np.mean(self.total_rewards),\n",
    "                                       global_step=epoch)\n",
    "\n",
    "                self.writer.add_scalar(tag='Entropy',\n",
    "                                       scalar_value=entropy,\n",
    "                                       global_step=epoch)\n",
    "\n",
    "                # reset the epoch arrays\n",
    "                # used for entropy calculation\n",
    "                epoch_logits = torch.empty(size=(0, self.env.action_space.n), device=self.DEVICE)\n",
    "                epoch_weighted_log_probs = torch.empty(size=(0,), dtype=torch.float, device=self.DEVICE)\n",
    "\n",
    "                # check if solved\n",
    "                if np.mean(self.total_rewards) > 200:\n",
    "                    print('\\nSolved!')\n",
    "                    break\n",
    "\n",
    "        # close the environment\n",
    "        self.env.close()\n",
    "\n",
    "        # close the writer\n",
    "        self.writer.close()\n",
    "\n",
    "    def play_episode(self, episode: int):\n",
    "        \"\"\"\n",
    "            Plays an episode of the environment.\n",
    "            episode: the episode counter\n",
    "            Returns:\n",
    "                sum_weighted_log_probs: the sum of the log-prob of an action multiplied by the reward-to-go from that state\n",
    "                episode_logits: the logits of every step of the episode - needed to compute entropy for entropy bonus\n",
    "                finished_rendering_this_epoch: pass-through rendering flag\n",
    "                sum_of_rewards: sum of the rewards for the episode - needed for the average over 200 episode statistic\n",
    "        \"\"\"\n",
    "        # reset the environment to a random initial state every epoch\n",
    "        state = self.env.reset()\n",
    "\n",
    "        # initialize the episode arrays\n",
    "        episode_actions = torch.empty(size=(0,), dtype=torch.long, device=self.DEVICE)\n",
    "        episode_logits = torch.empty(size=(0, self.env.action_space.n), device=self.DEVICE)\n",
    "        average_rewards = np.empty(shape=(0,), dtype=np.float)\n",
    "        episode_rewards = np.empty(shape=(0,), dtype=np.float)\n",
    "\n",
    "        # episode loop\n",
    "        while True:\n",
    "\n",
    "            # render the environment for the first episode in the epoch\n",
    "            if not self.finished_rendering_this_epoch:\n",
    "                #self.env.render()\n",
    "                pass\n",
    "                \n",
    "            # get the action logits from the agent - (preferences)\n",
    "            action_logits = self.agent(torch.tensor(state).float().unsqueeze(dim=0).to(self.DEVICE))\n",
    "\n",
    "            #print('action logits is',action_logits)\n",
    "            \n",
    "            # append the logits to the episode logits list\n",
    "            episode_logits = torch.cat((episode_logits, action_logits), dim=0)\n",
    "\n",
    "            # sample an action according to the action distribution\n",
    "            action = Categorical(logits=action_logits).sample()\n",
    "\n",
    "            #print('the action after categorical is',action)\n",
    "            \n",
    "            # append the action to the episode action list to obtain the trajectory\n",
    "            # we need to store the actions and logits so we could calculate the gradient of the performance\n",
    "            episode_actions = torch.cat((episode_actions, action), dim=0)\n",
    "\n",
    "            # take the chosen action, observe the reward and the next state\n",
    "            state, reward, done, _ = self.env.step(action=action.cpu().item())\n",
    "\n",
    "            # append the reward to the rewards pool that we collect during the episode\n",
    "            # we need the rewards so we can calculate the weights for the policy gradient\n",
    "            # and the baseline of average\n",
    "            episode_rewards = np.concatenate((episode_rewards, np.array([reward])), axis=0)\n",
    "\n",
    "            # here the average reward is state specific\n",
    "            average_rewards = np.concatenate((average_rewards,\n",
    "                                              np.expand_dims(np.mean(episode_rewards), axis=0)),\n",
    "                                             axis=0)\n",
    "\n",
    "            # the episode is over\n",
    "            if done:\n",
    "\n",
    "                # increment the episode\n",
    "                episode += 1\n",
    "\n",
    "                # turn the rewards we accumulated during the episode into the rewards-to-go:\n",
    "                # earlier actions are responsible for more rewards than the later taken actions\n",
    "                discounted_rewards_to_go = PolicyGradient.get_discounted_rewards(rewards=episode_rewards,\n",
    "                                                                                 gamma=self.GAMMA)\n",
    "                discounted_rewards_to_go -= average_rewards  # baseline - state specific average\n",
    "\n",
    "                # # calculate the sum of the rewards for the running average metric\n",
    "                sum_of_rewards = np.sum(episode_rewards)\n",
    "\n",
    "                # set the mask for the actions taken in the episode\n",
    "                mask = one_hot(episode_actions, num_classes=self.env.action_space.n)\n",
    "\n",
    "                # calculate the log-probabilities of the taken actions\n",
    "                # mask is needed to filter out log-probabilities of not related logits\n",
    "                episode_log_probs = torch.sum(mask.float() * log_softmax(episode_logits, dim=1), dim=1)\n",
    "\n",
    "                # weight the episode log-probabilities by the rewards-to-go\n",
    "                episode_weighted_log_probs = episode_log_probs * \\\n",
    "                    torch.tensor(discounted_rewards_to_go).float().to(self.DEVICE)\n",
    "\n",
    "                # calculate the sum over trajectory of the weighted log-probabilities\n",
    "                sum_weighted_log_probs = torch.sum(episode_weighted_log_probs).unsqueeze(dim=0)\n",
    "\n",
    "                # won't render again this epoch\n",
    "                self.finished_rendering_this_epoch = True\n",
    "\n",
    "                return sum_weighted_log_probs, episode_logits, sum_of_rewards, episode\n",
    "\n",
    "    def calculate_loss(self, epoch_logits: torch.Tensor, weighted_log_probs: torch.Tensor) -> (torch.Tensor, torch.Tensor):\n",
    "        \"\"\"\n",
    "            Calculates the policy \"loss\" and the entropy bonus\n",
    "            Args:\n",
    "                epoch_logits: logits of the policy network we have collected over the epoch\n",
    "                weighted_log_probs: loP * W of the actions taken\n",
    "            Returns:\n",
    "                policy loss + the entropy bonus\n",
    "                entropy: needed for logging\n",
    "        \"\"\"\n",
    "        policy_loss = -1 * torch.mean(weighted_log_probs)\n",
    "\n",
    "        # add the entropy bonus\n",
    "        p = softmax(epoch_logits, dim=1)\n",
    "        log_p = log_softmax(epoch_logits, dim=1)\n",
    "        entropy = -1 * torch.mean(torch.sum(p * log_p, dim=1), dim=0)\n",
    "        entropy_bonus = -1 * self.BETA * entropy\n",
    "\n",
    "        print('policy loss is',policy_loss + entropy_bonus)\n",
    "        \n",
    "        return policy_loss + entropy_bonus, entropy\n",
    "\n",
    "    @staticmethod\n",
    "    def get_discounted_rewards(rewards: np.array, gamma: float) -> np.array:\n",
    "        \"\"\"\n",
    "            Calculates the sequence of discounted rewards-to-go.\n",
    "            Args:\n",
    "                rewards: the sequence of observed rewards\n",
    "                gamma: the discount factor\n",
    "            Returns:\n",
    "                discounted_rewards: the sequence of the rewards-to-go\n",
    "        \"\"\"\n",
    "        discounted_rewards = np.empty_like(rewards, dtype=np.float)\n",
    "        for i in range(rewards.shape[0]):\n",
    "            gammas = np.full(shape=(rewards[i:].shape[0]), fill_value=gamma)\n",
    "            discounted_gammas = np.power(gammas, np.arange(rewards[i:].shape[0]))\n",
    "            discounted_reward = np.sum(rewards[i:] * discounted_gammas)\n",
    "            discounted_rewards[i] = discounted_reward\n",
    "        return discounted_rewards\n",
    "\n",
    "\n",
    "class main():\n",
    "#     args = parser.parse_args()\n",
    "#     env = args.env\n",
    "#     use_cuda = args.use_cuda\n",
    "\n",
    "#     assert(env in ['CartPole', 'LunarLander'])\n",
    "\n",
    "    env = 'CartPole'\n",
    "    use_cuda = True\n",
    "    \n",
    "    policy_gradient = PolicyGradient(problem=env, use_cuda=use_cuda)\n",
    "    policy_gradient.solve_environment()\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cba90ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_class = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f3ea62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('p37': conda)",
   "language": "python",
   "name": "python37464bitp37condace755f694755478c9bd95f16cbb10579"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
