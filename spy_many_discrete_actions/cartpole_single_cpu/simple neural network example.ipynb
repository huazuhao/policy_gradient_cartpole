{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fee243b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 5, Avg Return per Epoch: 19.300"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zuhao/anaconda2/envs/p37/lib/python3.7/site-packages/ipykernel_launcher.py:179: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "/home/zuhao/anaconda2/envs/p37/lib/python3.7/site-packages/ipykernel_launcher.py:180: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "/home/zuhao/anaconda2/envs/p37/lib/python3.7/site-packages/ipykernel_launcher.py:284: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 355, Avg Return per Epoch: 200.770\n",
      "Solved!\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.functional import one_hot, log_softmax, softmax, normalize\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import deque\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--env', help='CartPole or LunarLander OpenAI gym environment', type=str)\n",
    "parser.add_argument('--use_cuda', help='Use if you want to use CUDA', action='store_true')\n",
    "\n",
    "\n",
    "class Params:\n",
    "    NUM_EPOCHS = 10\n",
    "    ALPHA = 5e-3        # learning rate\n",
    "    BATCH_SIZE = 2      # how many episodes we want to pack into an epoch\n",
    "    GAMMA = 0.99        # discount rate\n",
    "    HIDDEN_SIZE = 64    # number of hidden nodes we have in our dnn\n",
    "    BETA = 0.1          # the entropy bonus multiplier\n",
    "\n",
    "\n",
    "# Q-table is replaced by a neural network\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, observation_space_size: int, action_space_size: int, hidden_size: int):\n",
    "        super(Agent, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features=observation_space_size, out_features=hidden_size, bias=True),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(in_features=hidden_size, out_features=hidden_size, bias=True),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(in_features=hidden_size, out_features=action_space_size, bias=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = normalize(x, dim=1)\n",
    "        x = self.net(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PolicyGradient:\n",
    "    def __init__(self, problem: str = \"CartPole\", use_cuda: bool = False):\n",
    "\n",
    "        self.NUM_EPOCHS = Params.NUM_EPOCHS\n",
    "        self.ALPHA = Params.ALPHA\n",
    "        self.BATCH_SIZE = Params.BATCH_SIZE\n",
    "        self.GAMMA = Params.GAMMA\n",
    "        self.HIDDEN_SIZE = Params.HIDDEN_SIZE\n",
    "        self.BETA = Params.BETA\n",
    "        self.DEVICE = torch.device('cuda' if torch.cuda.is_available() and use_cuda else 'cpu')\n",
    "\n",
    "        # instantiate the tensorboard writer\n",
    "        self.writer = SummaryWriter(comment=f'_PG_CP_Gamma={self.GAMMA},'\n",
    "                                            f'LR={self.ALPHA},'\n",
    "                                            f'BS={self.BATCH_SIZE},'\n",
    "                                            f'NH={self.HIDDEN_SIZE},'\n",
    "                                            f'BETA={self.BETA}')\n",
    "\n",
    "        # create the environment\n",
    "        self.env = gym.make('CartPole-v1') if problem == \"CartPole\" else gym.make('LunarLander-v2')\n",
    "\n",
    "        # the agent driven by a neural network architecture\n",
    "        self.agent = Agent(observation_space_size=self.env.observation_space.shape[0],\n",
    "                           action_space_size=self.env.action_space.n,\n",
    "                           hidden_size=self.HIDDEN_SIZE).to(self.DEVICE)\n",
    "\n",
    "        self.adam = optim.Adam(params=self.agent.parameters(), lr=self.ALPHA)\n",
    "\n",
    "        self.total_rewards = deque([], maxlen=200)\n",
    "\n",
    "        # flag to figure out if we have render a single episode current epoch\n",
    "        self.finished_rendering_this_epoch = False\n",
    "\n",
    "    def solve_environment(self):\n",
    "        \"\"\"\n",
    "            The main interface for the Policy Gradient solver\n",
    "        \"\"\"\n",
    "        # init the episode and the epoch\n",
    "        episode = 0\n",
    "        epoch = 0\n",
    "\n",
    "        # init the epoch arrays\n",
    "        # used for entropy calculation\n",
    "        epoch_logits = torch.empty(size=(0, self.env.action_space.n), device=self.DEVICE)\n",
    "        epoch_weighted_log_probs = torch.empty(size=(0,), dtype=torch.float, device=self.DEVICE)\n",
    "\n",
    "        while True:\n",
    "\n",
    "            # play an episode of the environment\n",
    "            (episode_weighted_log_prob_trajectory,\n",
    "             episode_logits,\n",
    "             sum_of_episode_rewards,\n",
    "             episode) = self.play_episode(episode=episode)\n",
    "\n",
    "            # after each episode append the sum of total rewards to the deque\n",
    "            self.total_rewards.append(sum_of_episode_rewards)\n",
    "\n",
    "            # append the weighted log-probabilities of actions\n",
    "            epoch_weighted_log_probs = torch.cat((epoch_weighted_log_probs, episode_weighted_log_prob_trajectory),\n",
    "                                                 dim=0)\n",
    "\n",
    "            # append the logits - needed for the entropy bonus calculation\n",
    "            epoch_logits = torch.cat((epoch_logits, episode_logits), dim=0)\n",
    "\n",
    "            # if the epoch is over - we have epoch trajectories to perform the policy gradient\n",
    "            if episode >= self.BATCH_SIZE:\n",
    "\n",
    "                # reset the rendering flag\n",
    "                self.finished_rendering_this_epoch = False\n",
    "\n",
    "                # reset the episode count\n",
    "                episode = 0\n",
    "\n",
    "                # increment the epoch\n",
    "                epoch += 1\n",
    "\n",
    "                # calculate the loss\n",
    "                loss, entropy = self.calculate_loss(epoch_logits=epoch_logits,\n",
    "                                                    weighted_log_probs=epoch_weighted_log_probs)\n",
    "\n",
    "                # zero the gradient\n",
    "                self.adam.zero_grad()\n",
    "\n",
    "                # backprop\n",
    "                loss.backward()\n",
    "\n",
    "                # update the parameters\n",
    "                self.adam.step()\n",
    "\n",
    "                # feedback\n",
    "                print(\"\\r\", f\"Epoch: {epoch}, Avg Return per Epoch: {np.mean(self.total_rewards):.3f}\",\n",
    "                      end=\"\",\n",
    "                      flush=True)\n",
    "\n",
    "                self.writer.add_scalar(tag='Average Return over 100 episodes',\n",
    "                                       scalar_value=np.mean(self.total_rewards),\n",
    "                                       global_step=epoch)\n",
    "\n",
    "                self.writer.add_scalar(tag='Entropy',\n",
    "                                       scalar_value=entropy,\n",
    "                                       global_step=epoch)\n",
    "\n",
    "                # reset the epoch arrays\n",
    "                # used for entropy calculation\n",
    "                epoch_logits = torch.empty(size=(0, self.env.action_space.n), device=self.DEVICE)\n",
    "                epoch_weighted_log_probs = torch.empty(size=(0,), dtype=torch.float, device=self.DEVICE)\n",
    "\n",
    "                # check if solved\n",
    "                if np.mean(self.total_rewards) > 200:\n",
    "                    print('\\nSolved!')\n",
    "                    break\n",
    "\n",
    "        # close the environment\n",
    "        self.env.close()\n",
    "\n",
    "        # close the writer\n",
    "        self.writer.close()\n",
    "\n",
    "    def play_episode(self, episode: int):\n",
    "        \"\"\"\n",
    "            Plays an episode of the environment.\n",
    "            episode: the episode counter\n",
    "            Returns:\n",
    "                sum_weighted_log_probs: the sum of the log-prob of an action multiplied by the reward-to-go from that state\n",
    "                episode_logits: the logits of every step of the episode - needed to compute entropy for entropy bonus\n",
    "                finished_rendering_this_epoch: pass-through rendering flag\n",
    "                sum_of_rewards: sum of the rewards for the episode - needed for the average over 200 episode statistic\n",
    "        \"\"\"\n",
    "        # reset the environment to a random initial state every epoch\n",
    "        state = self.env.reset()\n",
    "\n",
    "        # initialize the episode arrays\n",
    "        episode_actions = torch.empty(size=(0,), dtype=torch.long, device=self.DEVICE)\n",
    "        episode_logits = torch.empty(size=(0, self.env.action_space.n), device=self.DEVICE)\n",
    "        average_rewards = np.empty(shape=(0,), dtype=np.float)\n",
    "        episode_rewards = np.empty(shape=(0,), dtype=np.float)\n",
    "\n",
    "        # episode loop\n",
    "        while True:\n",
    "\n",
    "            # render the environment for the first episode in the epoch\n",
    "            if not self.finished_rendering_this_epoch:\n",
    "                #self.env.render()\n",
    "                pass\n",
    "                \n",
    "            # get the action logits from the agent - (preferences)\n",
    "            action_logits = self.agent(torch.tensor(state).float().unsqueeze(dim=0).to(self.DEVICE))\n",
    "\n",
    "            #print('action logits is',action_logits)\n",
    "            \n",
    "            # append the logits to the episode logits list\n",
    "            episode_logits = torch.cat((episode_logits, action_logits), dim=0)\n",
    "\n",
    "            # sample an action according to the action distribution\n",
    "            action = Categorical(logits=action_logits).sample()\n",
    "\n",
    "            #print('the action after categorical is',action)\n",
    "            \n",
    "            # append the action to the episode action list to obtain the trajectory\n",
    "            # we need to store the actions and logits so we could calculate the gradient of the performance\n",
    "            episode_actions = torch.cat((episode_actions, action), dim=0)\n",
    "\n",
    "            # take the chosen action, observe the reward and the next state\n",
    "            state, reward, done, _ = self.env.step(action=action.cpu().item())\n",
    "\n",
    "            # append the reward to the rewards pool that we collect during the episode\n",
    "            # we need the rewards so we can calculate the weights for the policy gradient\n",
    "            # and the baseline of average\n",
    "            episode_rewards = np.concatenate((episode_rewards, np.array([reward])), axis=0)\n",
    "\n",
    "            # here the average reward is state specific\n",
    "            average_rewards = np.concatenate((average_rewards,\n",
    "                                              np.expand_dims(np.mean(episode_rewards), axis=0)),\n",
    "                                             axis=0)\n",
    "\n",
    "            # the episode is over\n",
    "            if done:\n",
    "\n",
    "                # increment the episode\n",
    "                episode += 1\n",
    "\n",
    "                # turn the rewards we accumulated during the episode into the rewards-to-go:\n",
    "                # earlier actions are responsible for more rewards than the later taken actions\n",
    "                discounted_rewards_to_go = PolicyGradient.get_discounted_rewards(rewards=episode_rewards,\n",
    "                                                                                 gamma=self.GAMMA)\n",
    "                discounted_rewards_to_go -= average_rewards  # baseline - state specific average\n",
    "\n",
    "                # # calculate the sum of the rewards for the running average metric\n",
    "                sum_of_rewards = np.sum(episode_rewards)\n",
    "\n",
    "                # set the mask for the actions taken in the episode\n",
    "                mask = one_hot(episode_actions, num_classes=self.env.action_space.n)\n",
    "\n",
    "                # calculate the log-probabilities of the taken actions\n",
    "                # mask is needed to filter out log-probabilities of not related logits\n",
    "                episode_log_probs = torch.sum(mask.float() * log_softmax(episode_logits, dim=1), dim=1)\n",
    "\n",
    "                # weight the episode log-probabilities by the rewards-to-go\n",
    "                episode_weighted_log_probs = episode_log_probs * \\\n",
    "                    torch.tensor(discounted_rewards_to_go).float().to(self.DEVICE)\n",
    "\n",
    "                # calculate the sum over trajectory of the weighted log-probabilities\n",
    "                sum_weighted_log_probs = torch.sum(episode_weighted_log_probs).unsqueeze(dim=0)\n",
    "\n",
    "                # won't render again this epoch\n",
    "                self.finished_rendering_this_epoch = True\n",
    "\n",
    "                return sum_weighted_log_probs, episode_logits, sum_of_rewards, episode\n",
    "\n",
    "    def calculate_loss(self, epoch_logits: torch.Tensor, weighted_log_probs: torch.Tensor) -> (torch.Tensor, torch.Tensor):\n",
    "        \"\"\"\n",
    "            Calculates the policy \"loss\" and the entropy bonus\n",
    "            Args:\n",
    "                epoch_logits: logits of the policy network we have collected over the epoch\n",
    "                weighted_log_probs: loP * W of the actions taken\n",
    "            Returns:\n",
    "                policy loss + the entropy bonus\n",
    "                entropy: needed for logging\n",
    "        \"\"\"\n",
    "        policy_loss = -1 * torch.mean(weighted_log_probs)\n",
    "\n",
    "        # add the entropy bonus\n",
    "        p = softmax(epoch_logits, dim=1)\n",
    "        log_p = log_softmax(epoch_logits, dim=1)\n",
    "        entropy = -1 * torch.mean(torch.sum(p * log_p, dim=1), dim=0)\n",
    "        entropy_bonus = -1 * self.BETA * entropy\n",
    "\n",
    "        return policy_loss + entropy_bonus, entropy\n",
    "\n",
    "    @staticmethod\n",
    "    def get_discounted_rewards(rewards: np.array, gamma: float) -> np.array:\n",
    "        \"\"\"\n",
    "            Calculates the sequence of discounted rewards-to-go.\n",
    "            Args:\n",
    "                rewards: the sequence of observed rewards\n",
    "                gamma: the discount factor\n",
    "            Returns:\n",
    "                discounted_rewards: the sequence of the rewards-to-go\n",
    "        \"\"\"\n",
    "        discounted_rewards = np.empty_like(rewards, dtype=np.float)\n",
    "        for i in range(rewards.shape[0]):\n",
    "            gammas = np.full(shape=(rewards[i:].shape[0]), fill_value=gamma)\n",
    "            discounted_gammas = np.power(gammas, np.arange(rewards[i:].shape[0]))\n",
    "            discounted_reward = np.sum(rewards[i:] * discounted_gammas)\n",
    "            discounted_rewards[i] = discounted_reward\n",
    "        return discounted_rewards\n",
    "\n",
    "\n",
    "class main():\n",
    "#     args = parser.parse_args()\n",
    "#     env = args.env\n",
    "#     use_cuda = args.use_cuda\n",
    "\n",
    "#     assert(env in ['CartPole', 'LunarLander'])\n",
    "\n",
    "    env = 'CartPole'\n",
    "    use_cuda = True\n",
    "    \n",
    "    policy_gradient = PolicyGradient(problem=env, use_cuda=use_cuda)\n",
    "    policy_gradient.solve_environment()\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cba90ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " Epoch: 1, Avg Return per Epoch: 25.500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zuhao/anaconda2/envs/p37/lib/python3.7/site-packages/ipykernel_launcher.py:179: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "/home/zuhao/anaconda2/envs/p37/lib/python3.7/site-packages/ipykernel_launcher.py:180: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "/home/zuhao/anaconda2/envs/p37/lib/python3.7/site-packages/ipykernel_launcher.py:284: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 175, Avg Return per Epoch: 201.490\n",
      "Solved!\n"
     ]
    }
   ],
   "source": [
    "main_class = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07639e85",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'agent'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-10a48928088c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'agent'"
     ]
    }
   ],
   "source": [
    "main_class.agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390823a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('p37': conda)",
   "language": "python",
   "name": "python37464bitp37condace755f694755478c9bd95f16cbb10579"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
